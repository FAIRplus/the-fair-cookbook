{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(fcb-notebook-1)=# Starting material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "````{panels}\n",
    ":container: container-lg pb-3<br>\n",
    ":column: col-lg-3 col-md-4 col-sm-6 col-xs-12 p-1<br>\n",
    ":card: rounded<br>\n",
    "\n",
    "<i class='fa fa-qrcode fa-2x' style='color:#7e0038;'></i><br>\n",
    "^^^\n",
    "<h4><b>Recipe metadata</b></h4>\n",
    " identifier: <a href=\"\">RX.X</a>\n",
    " version: <a href=\"\">v1.0</a>\n",
    "\n",
    "---\n",
    "<i class='fa fa-fire fa-2x' style='color:#7e0038;'></i>\n",
    "^^^\n",
    "<h4><b>Difficulty level</b></h4>\n",
    "<i class='fa fa-fire fa-lg' style='color:#7e0038;'></i>\n",
    "<i class='fa fa-fire fa-lg' style='color:#7e0038;'></i>\n",
    "<i class='fa fa-fire fa-lg' style='color:#7e0038;'></i>\n",
    "<i class='fa fa-fire fa-lg' style='color:#7e0038;'></i>\n",
    "<i class='fa fa-fire fa-lg' style='color:lightgrey'></i>\n",
    "\n",
    "---\n",
    "<i class='fas fa-clock fa-2x' style='color:#7e0038;'></i>\n",
    "^^^\n",
    "<h4><b>Reading Time</b></h4>\n",
    "<i class='fa fa-clock fa-lg' style='color:#7e0038;'></i> 30 minutes\n",
    "<h4><b>Recipe Type</b></h4>\n",
    "<i class='fa fa-globe fa-lg' style='color:#7e0038;'></i> Hands-on\n",
    "<h4><b>Executable Code</b></h4>\n",
    "<i class='fa fa-play-circle fa-lg' style='color:#7e0038;'></i> Yes\n",
    "\n",
    "---\n",
    "<i class='fa fa-users fa-2x' style='color:#7e0038;'></i>\n",
    "^^^\n",
    "<h4><b>Intended Audience</b></h4>\n",
    "<!-- <p> <i class='fa fa-user-md fa-lg' style='color:#7e0038;'></i> Principal Investigator </p> -->\n",
    "<p> <i class='fa fa-database fa-lg' style='color:#7e0038;'></i> Data Manager </p>\n",
    "<p> <i class='fa fa-wrench fa-lg' style='color:#7e0038;'></i> Data Scientist </p>\n",
    "````\n",
    "---\n",
    "\n",
    "**title:** ` ` <br>\n",
    "**author:** `philippe.rocca-serra@oerc.ox.ac.uk` <br>\n",
    "**date:** `2020-05-11` <br>\n",
    "**license:** [`creative-commons-by-sa-4.0`](https://creativecommons.org/licenses/by/4.0/) <br>\n",
    "**keywords:** ` ` <br>\n",
    "**description:** ` ` <br>\n",
    "\n",
    "Background: \n",
    "\n",
    "Experimental results such as metabolite profiling data published in [1,2] can be straightfowardly reported using [OKFN Data Packages](https://frictionlessdata.io/specs/tabular-data-package/). Such components can be easily parsed as data frames and exploiting for data visualization purpose using libraries implementing graphical grammar concepts. Here, we show how to use a set of python libraries to create a tabular data package from an excel file, annotate it with ontologies (CHEBI, PO, NCBITax) and validate the results against the JSON definition of the data table.\n",
    "A few line of codes allow structure information aound key study design descriptors: the independent variables and their levels have been clearly and unambiguously declared in the Tabular Data Package itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Let's begin by installing the Python packages allowing easy access and use of data formatted as JSON Data Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import libchebipy\n",
    "import re\n",
    "import pandas as pd\n",
    "from datapackage import Package\n",
    "from goodtables import validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Reading the data:\n",
    "\n",
    "We now simply read in the Excel file corresponding to the Nature Genetics Supplementary Table from the Zenodo archive\n",
    "\n",
    "(DOI: https://doi.org/10.5281/zenodo.2598799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df = pd.read_excel('Supplementary Data 3.xlsx', sheet_name='Feuil1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('https://zenodo.org/api/files/91a610cb-8f1f-4ec5-9818-767a75a7a820/Supplementary%20Data%203.xlsx', sheet_name='Feuil1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Following a manual inspection of the Excel Source, getting the start row of the data, we use Pandas take() function to extract first a row of headers (hence -axis set to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "header_treatment = df.take([13], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We then extract all the columns of interest (same take() function, with -axis set to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data_full = df.take([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], axis=1)\n",
    "# We now trim by removing the first 15 rows which contain no information\n",
    "data_slice = data_full.take([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
    "                             39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61,\n",
    "                             62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We now rename the DataFrame automatically generated field header to something more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_slice.rename(columns={\"Unnamed: 3\": \"chemical_name\",\n",
    "                           \"Unnamed: 4\": \"sample_mean_1\",\n",
    "                           \"Unnamed: 5\": \"sem_1\",\n",
    "                           \"Unnamed: 6\": \"sample_mean_2\",\n",
    "                           \"Unnamed: 7\": \"sem_2\",\n",
    "                           \"Unnamed: 8\": \"sample_mean_3\",\n",
    "                           \"Unnamed: 9\": \"sem_3\",\n",
    "                           \"Unnamed: 10\": \"sample_mean_4\",\n",
    "                           \"Unnamed: 11\": \"sem_4\",\n",
    "                           \"Unnamed: 12\": \"sample_mean_5\",\n",
    "                           \"Unnamed: 13\": \"sem_5\",\n",
    "                           \"Unnamed: 14\": \"sample_mean_6\",\n",
    "                           \"Unnamed: 15\": \"sem_6\",\n",
    "                           \"Unnamed: 16\": \"sample_mean_7\",\n",
    "                           \"Unnamed: 17\": \"sem_7\",\n",
    "                           \"Unnamed: 18\": \"sample_mean_8\",\n",
    "                           \"Unnamed: 19\": \"sem_8\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. inserting 2 new fields as placeholders for chemical information descriptors and we reinitialize the dataframe index so row numbering start at 0, not 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_slice.insert(loc=1, column='inchi', value='')\n",
    "data_slice.insert(loc=2, column='chebi_identifier', value='')\n",
    "data_slice = data_slice.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using LibChebi to retrieve CHEBI identifiers and InChi from a chemical name. Note: in this call, we retrieve only values for which an exact match on the chemical name is found in Chebi libchebi API does not allow easy searching on synonyms, thus we are failing to retrieve all relevant information. This is merely to showcase how to use libchebi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, 60):\n",
    "    hit = libchebipy.search(data_slice.loc[i, 'chemical_name'], True)\n",
    "    if len(hit) > 0:\n",
    "        print(\"HIT: \", data_slice.loc[i, 'chemical_name'], \":\", hit[0].get_inchi(), \"|\", hit[0].get_id())\n",
    "        data_slice.loc[i, 'inchi'] = hit[0].get_inchi()\n",
    "        data_slice.loc[i, 'chebi_identifier'] = hit[0].get_id()\n",
    "    else:\n",
    "        print(\"Nothing found: \", data_slice.loc[i, 'chemical_name'])\n",
    "        data_slice.loc[i, 'inchi'] = ''\n",
    "        data_slice.loc[i, 'chebi_identifier'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. The following steps are needed to perform the table transformation from a 'wide' layout to a 'long table' one. Prep stubnames - pick out all the feature_model variables and remove the model suffices 'long table' layout is that relied on by Frictionless Tabular Data Packages and consumed by R ggplot2 library and Python plotnine library.\n",
    "Step1: obtain all the different 'dimension' measured for a given condition (i.e. repeating fields with an increment suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_models = [col for col in data_slice.columns if re.match(\"(sample_mean|sem)_[0-9]\", col) is not None]\n",
    "features = list(set([re.sub(\"_[0-9]\", \"\", feature_model) for feature_model in feature_models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Step2: invoke Pandas pd.wide_to_long() function to carry out the table transformation. See Pandas documentation for more information:  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html and the excellent blog: https://medium.com/@wangyuw/data-reshaping-with-pandas-explained-80b2f51f88d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_df = pd.wide_to_long(data_slice, i=['chemical_name'], j='treatment', stubnames=features, sep=\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Apparently a feature in Pandas DataFrame causes an mismatch in the field position. We solve this by writing the DataFrame to file and reading it back in again, not ideal but it does the trick. Writing to a temporary file &  reading from that file to solve the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_df.to_csv(\"long.txt\", sep='\\t', encoding='utf-8')\n",
    "long_df_from_file = pd.read_csv(\"long.txt\", sep=\"\\t\")\n",
    "long_df_from_file.head()\n",
    "\n",
    "try:\n",
    "    os.remove(\"long.txt\")\n",
    "except IOError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Insert a new field 'unit' in the DataFrame at position 3 and setting value to empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_df_from_file.insert(loc=3, column='unit', value='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Adding new fields for each of the independent variable and associated URI, copying from 'treatment field'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_df_from_file['var1_levels'] = long_df_from_file['treatment']\n",
    "long_df_from_file['var1_uri'] = long_df_from_file['treatment']\n",
    "long_df_from_file['var2_levels'] = long_df_from_file['treatment']\n",
    "long_df_from_file['var2_uri'] = long_df_from_file['treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adding a new field for 'sample size' and setting the value to n=3\n",
    "long_df_from_file['sample_size'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Marking up with ontology terms and their resolvable URI for all factor values. This requires doing a manual mapping, better ways could be devised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_df_from_file.loc[long_df_from_file['treatment'] == 1, 'treatment'] = 'R. chinensis \\'Old Blush\\' sepals'\n",
    "long_df_from_file.loc[long_df_from_file['var1_levels'] == 1, 'var1_levels'] = 'R. chinensis \\'Old Blush\\''\n",
    "long_df_from_file.loc[long_df_from_file['var1_uri'] == 1, 'var1_uri'] = 'http://purl.obolibrary.org/obo/NCBITaxon_74649'\n",
    "long_df_from_file.loc[long_df_from_file['var2_levels'] == 1, 'var2_levels'] = 'sepals'\n",
    "long_df_from_file.loc[long_df_from_file['var2_uri'] == 1, 'var2_uri'] = 'http://purl.obolibrary.org/obo/PO_0009031'\n",
    "\n",
    "long_df_from_file.loc[long_df_from_file['treatment'] == 2, 'treatment'] = 'R. chinensis \\'Old Blush\\' stamens'\n",
    "long_df_from_file.loc[long_df_from_file['var1_levels'] == 2, 'var1_levels'] = 'R. chinensis \\'Old Blush\\''\n",
    "long_df_from_file.loc[long_df_from_file['var1_uri'] == 2, 'var1_uri'] = 'http://purl.obolibrary.org/obo/NCBITaxon_74649'\n",
    "long_df_from_file.loc[long_df_from_file['var2_levels'] == 2, 'var2_levels'] = 'stamens'\n",
    "long_df_from_file.loc[long_df_from_file['var2_uri'] == 2, 'var2_uri'] = 'http://purl.obolibrary.org/obo/PO_0009029'\n",
    "\n",
    "long_df_from_file.loc[long_df_from_file['treatment'] == 3, 'treatment'] = 'R. chinensis \\'Old Blush\\' petals'\n",
    "long_df_from_file.loc[long_df_from_file['var1_levels'] == 3, 'var1_levels'] = 'R. chinensis \\'Old Blush\\''\n",
    "long_df_from_file.loc[long_df_from_file['var1_uri'] == 3, 'var1_uri'] = 'http://purl.obolibrary.org/obo/NCBITaxon_74649'\n",
    "long_df_from_file.loc[long_df_from_file['var2_levels'] == 3, 'var2_levels'] = 'petals'\n",
    "long_df_from_file.loc[long_df_from_file['var2_uri'] == 3, 'var2_uri'] = 'http://purl.obolibrary.org/obo/PO_0009032'\n",
    "\n",
    "long_df_from_file.loc[long_df_from_file['treatment'] == 4, 'treatment'] = 'R. gigantea petals'\n",
    "long_df_from_file.loc[long_df_from_file['var1_levels'] == 4, 'var1_levels'] = 'R. gigantea'\n",
    "long_df_from_file.loc[long_df_from_file['var1_uri'] == 4, 'var1_uri'] = 'http://purl.obolibrary.org/obo/NCBITaxon_74650'\n",
    "long_df_from_file.loc[long_df_from_file['var2_levels'] == 4, 'var2_levels'] = 'petals'\n",
    "long_df_from_file.loc[long_df_from_file['var2_uri'] == 4, 'var2_uri'] = 'http://purl.obolibrary.org/obo/PO_0009032'\n",
    "\n",
    "long_df_from_file.loc[long_df_from_file['treatment'] == 5, 'treatment'] = 'R. Damascena petals'\n",
    "long_df_from_file.loc[long_df_from_file['var1_levels'] == 5, 'var1_levels'] = 'R. Damascena'\n",
    "long_df_from_file.loc[long_df_from_file['var1_uri'] == 5, 'var1_uri'] = 'http://purl.obolibrary.org/obo/NCBITaxon_3765'\n",
    "long_df_from_file.loc[long_df_from_file['var2_levels'] == 5, 'var2_levels'] = 'petals'\n",
    "long_df_from_file.loc[long_df_from_file['var2_uri'] == 5, 'var2_uri'] = 'http://purl.obolibrary.org/obo/PO_0009032'\n",
    "\n",
    "long_df_from_file.loc[long_df_from_file['treatment'] == 6, 'treatment'] = 'R. Gallica petals'\n",
    "long_df_from_file.loc[long_df_from_file['var1_levels'] == 6, 'var1_levels'] = 'R. Gallica'\n",
    "long_df_from_file.loc[long_df_from_file['var1_uri'] == 6, 'var1_uri'] = 'http://purl.obolibrary.org/obo/NCBITaxon_74632'\n",
    "long_df_from_file.loc[long_df_from_file['var2_levels'] == 6, 'var2_levels'] = 'petals'\n",
    "long_df_from_file.loc[long_df_from_file['var2_uri'] == 6, 'var2_uri'] = 'http://purl.obolibrary.org/obo/PO_0009032'\n",
    "\n",
    "long_df_from_file.loc[long_df_from_file['treatment'] == 7, 'treatment'] = 'R. moschata petals'\n",
    "long_df_from_file.loc[long_df_from_file['var1_levels'] == 7, 'var1_levels'] = 'R. moschata'\n",
    "long_df_from_file.loc[long_df_from_file['var1_uri'] == 7, 'var1_uri'] = 'http://purl.obolibrary.org/obo/NCBITaxon_74646'\n",
    "long_df_from_file.loc[long_df_from_file['var2_levels'] == 7, 'var2_levels'] = 'petals'\n",
    "long_df_from_file.loc[long_df_from_file['var2_uri'] == 7, 'var2_uri'] = 'http://purl.obolibrary.org/obo/PO_0009032'\n",
    "\n",
    "long_df_from_file.loc[long_df_from_file['treatment'] == 8, 'treatment'] = 'R. wichurana petals'\n",
    "long_df_from_file.loc[long_df_from_file['var1_levels'] == 8, 'var1_levels'] = 'R. wichurana'\n",
    "long_df_from_file.loc[long_df_from_file['var1_uri'] == 8, 'var1_uri'] = 'http://purl.obolibrary.org/obo/NCBITaxon_2094184'\n",
    "long_df_from_file.loc[long_df_from_file['var2_levels'] == 8, 'var2_levels'] = 'petals'\n",
    "long_df_from_file.loc[long_df_from_file['var2_uri'] == 8, 'var2_uri'] = 'http://purl.obolibrary.org/obo/PO_0009032'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Dealing with missing values: setting empty values to zero for sample_mean and sem to enable calculation: to do this, we rely on Pandas fillna function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_df_from_file['sample_mean'] = long_df_from_file['sample_mean'].fillna(\"0\")\n",
    "long_df_from_file['sem'] = long_df_from_file['sample_mean'].fillna(\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Reorganizing Columns order in the DataFrame/File to match the Frictionless Tabular Data Package Layout. This is done very easily in Pandas by passing desired column order as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "long_df_from_file = long_df_from_file[['chemical_name', 'inchi', 'chebi_identifier', 'var1_levels', 'var1_uri',\n",
    "                                       'var2_levels', 'var2_uri', 'treatment', 'sample_size', 'sample_mean',\n",
    "                                       'unit', 'sem']]\n",
    "long_df_from_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. We are now ready to write the file to disk as a UTF-8 encoded comma delimited file, with double quoted values and we are also dropping the dataframe index from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    HOME=os.getcwd()\n",
    "    # print(\"checking current directory #1: \",HOME)\n",
    "\n",
    "    if not os.path.exists(os.path.join(HOME,'../data/processed/denovo')):\n",
    "        # print(\"checking current directory #2: \", os.getcwd())\n",
    "        os.makedirs(os.path.join(HOME,'../data/processed/denovo'))\n",
    "        os.chdir(os.path.join(HOME,'../data/processed/denovo'))\n",
    "        long_df_from_file.to_csv(\"rose-aroma-naturegenetics2018-treatment-group-mean-sem-report-table-example.csv\",\n",
    "                         quoting=1,\n",
    "                         doublequote=True, sep=',',\n",
    "                         encoding='utf-8', index=False)\n",
    "    else:\n",
    "        os.chdir(os.path.join(HOME,'../data/processed/denovo'))\n",
    "\n",
    "except IOError as e:\n",
    "    print(e)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. The Final step is to validate the output against JSON data package specifications, which are stored in the JSON Tabular DataPackage Definition folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir('./../../../')\n",
    "LOCAL = os.getcwd()\n",
    "print(\"moving to directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "package_definition = os.path.join(LOCAL,'./rose-metabo-JSON-DP-validated/rose-aroma-naturegenetics2018-treatment-group-mean-sem-report-datapackage.json')\n",
    "file_to_test = os.path.join(LOCAL,'../data/processed/denovo/rose-aroma-naturegenetics2018-treatment-group-mean-sem-report-table-example.csv')\n",
    "\n",
    "print (\"JSON data package definition:\", package_definition)\n",
    "print(\"csv file to evaluate:\", file_to_test)\n",
    "try:\n",
    "    pack = Package(package_definition)\n",
    "    pack.valid\n",
    "    pack.errors\n",
    "    for e in pack.errors:\n",
    "        print(e)\n",
    "\n",
    "    report = validate(file_to_test)\n",
    "    if report['valid']== True:\n",
    "        print(\"Success! \\n\") \n",
    "        print(\"\\'\"+file_to_test + \"\\'\"+ \" is a valid Frictionless Tabular Data Package\\n\" + \"It complies with the 'rose-aroma-naturegenetics2018-treatment-group-mean-sem-report-datapackage.json' definition\\n\")\n",
    "    else:\n",
    "        print(\"hmmm, something went wrong. Please, see the validation report for tracing the fault\")\n",
    "\n",
    "except IOError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. This concludes this notebook, which showed you how to convert a metabolite profiling dataset from a publication and create a FAIR data package. The other notebooks will show you how to visualize and plot the dataset but also convert it to a semantic graph as a Linked Data representation, query it and plot from it.\n",
    "    - [1-rose-metabolites-Python-analysis.ipynb](../notebooks/1-rose-metabolites-Python-analysis.ipynb)\n",
    "    - [2-rose-metabolites-Python-RDF-querying-analysis.ipynb](../notebooks/2-rose-metabolites-Python-RDF-querying-analysis.ipynb)\n",
    "    \n",
    "    - [3-rose-metabolites-R-analysis.ipynb](../notebooks/3-rose-metabolites-R-analysis.ipynb) \n",
    "    (NB: requires making an R kernel available to Jupyter)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bibliographic Reference:\n",
    "\n",
    "[1.Magnard JL, Roccia A, Caissard JC, Vergne P, Sun P, Hecquet R, Dubois A, Hibrand-Saint Oyant L, Jullien F, Nicolè F, Raymond O, Huguet S, Baltenweck R, Meyer S, Claudel P, Jeauffre J, Rohmer M, Foucher F, Hugueney P, Bendahmane M, Baudino S. PLANT VOLATILES. Biosynthesis of monoterpene scent compounds in roses. Science. 2015 Jul 3;349(6243):81-3.](https://doi.org/10.1126/science.aab0696)\n",
    "\n",
    "[2.Raymond O, Gouzy J, Just J, Badouin H, Verdenaud M, Lemainque A, Vergne P, Moja S, Choisne N, Pont C, Carrère S, Caissard JC, Couloux A, Cottret L, Aury JM, Szécsi J, Latrasse D, Madoui MA, François L, Fu X, Yang SH, Dubois A, Piola F, Larrieu A, Perez M, Labadie K, Perrier L, Govetto B, Labrousse Y, Villand P, Bardoux C, Boltz V, Lopez-Roques C, Heitzler P, Vernoux T, Vandenbussche M, Quesneville H, Boualem A, Bendahmane A, Liu C, Le Bris M, Salse J, Baudino S, Benhamed M, Wincker P, Bendahmane M. The Rosa genome provides new insights into the domestication of modern roses. Nat Genet. 2018 Jun;50(6):772-777.](https://doi.org/10.1038/s41588-018-0110-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv372",
   "language": "python",
   "name": "venv372"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
