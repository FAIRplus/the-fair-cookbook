{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te0RwWgoziNV"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "If you ever had to do a literature search for a project, you probably could appreciate the great effort behind traversing the ever-expanding volumes of texts and trying to organize the extracted information. Throughout the last decades, some noticeable progress was made in using AI to automize the process. The modern machine learning approaches aim to identify, extract and store important information from unstructured texts. To make the extracted metadata active and FAIR, one often stores it in the form of a knowledge graph.\n",
    "\n",
    "The pipeline for information extraction could be seen as a path of several steps:\n",
    "\n",
    "- Collecting the text data.\n",
    "- Avoiding ambiguity of entities with co-reference resolution.\n",
    "- Entity recognition and named entity linking.\n",
    "- Relationship extraction.\n",
    "- Storing the data as a knowledge graph.\n",
    "\n",
    "## Collecting the text data\n",
    "\n",
    "First, one collects the text to extract the data from. Text may be the collection of internal documents, articles, online content, or the result of picture descriptions produced by image-to-text algorithms.\n",
    "\n",
    "Here as an example, we will collect a dataset of articles' abstracts on the topic \"cardiac amyloidosis\". In the biological domain, articles can be collected from the PubMed database using biopython, for the sake of simplicity we will only go through the first 20 articles that come up in the search.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zmel-dIjGzDa",
    "outputId": "b167e8af-b54c-4adb-a290-52bfb22201c9"
   },
   "outputs": [],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3z7-zuyj0DY3",
    "outputId": "2b78f467-ddb3-420e-c4c7-c3ab3a4f41a6"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from Bio import Entrez\n",
    "\n",
    "def search(query, max_papers=20):\n",
    "    \"\"\"\n",
    "    Get IDs of papers on the given topic from the pubmed database.\n",
    "    \"\"\"\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax=max_papers,\n",
    "                            retmode='xml',\n",
    "                            term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "    \n",
    "def fetch_details(id_list):\n",
    "    \"\"\"\n",
    "    Get details on each paper (including the abstract).\n",
    "    \"\"\"\n",
    "    ids = ','.join(id_list)\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "    \n",
    "results = search('cardiac amyloidosis')\n",
    "id_list = results['IdList']\n",
    "papers = fetch_details(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2db344C6z1Hx"
   },
   "source": [
    "## Avoiding ambiguity of entities with coreference resolution\n",
    "The prepared text should go through the coreference resolution model. \n",
    "In a nutshell, this process should replace all ambiguous words in a sentence so that the text doesn’t need any extra context to be understood. For example, personal pronouns are being replaced with a referred person’s name. \n",
    "Although a number of approaches exist to perform the task, one of the most recently developed is [crosslingual coreference](https://spacy.io/universe/project/crosslingualcoreference) from the spaCy universe.\n",
    "spaCy is a python library that provides an easy way to create pipelines for natural language processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2EauxUukzyBg",
    "outputId": "872273c1-4870-4f5d-dd46-3804b9e12775"
   },
   "outputs": [],
   "source": [
    "!pip install crosslingual-coreference==0.2.3 spacy-transformers==1.1.5 wikipedia neo4j\n",
    "!pip install --upgrade google-cloud-storage\n",
    "!pip install transformers==4.18.0\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrJEH5Biz5M_",
    "outputId": "994dbbbc-510d-4909-a710-05e676c0df92"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import crosslingual_coreference\n",
    "\n",
    "# Configure the `Device` parameter:\n",
    "DEVICE = -1 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model:\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djLki28g0KyL"
   },
   "source": [
    "## Entity recognition and named entity linking\n",
    "The next step is known as `named entity recognition` (NER). \n",
    "Here, we want to extract all important entities from the sentences.\n",
    "Depending on the use case, one may need to train a model to recognize entities of a specific type.\n",
    "For example, [this tutorial](https://towardsdatascience.com/clinical-named-entity-recognition-using-spacy-5ae9c002e86f) details a way to train a model to recognize some entities from a biomedical domain. \n",
    "However, the spaCy universe also provides some pre-trained models to recognize entities, which we are going to use in our example.\n",
    "\n",
    "Then, one needs to standardize the entities and map them to an existing ontology.\n",
    "The process is known as `entity linking`. \n",
    "Here, we map entities from the text to corresponding unique identifiers from a target knowledge base, for example, Wikipedia.\n",
    "One can also use databases relevant to the specific topic of the texts.\n",
    "We will try to map our entities to the [NCI Thesarius](https://bioportal.bioontology.org/ontologies/NCIT), for simplicity choosing the first match as a mapping.\n",
    "\n",
    "Note, that in principle that is not always the best choice and one can use different similarity metrics to identify the best matching term in the ontology. \n",
    "\n",
    "A mapping to the Wikipedia terms is performed in [this tutorial](https://towardsdatascience.com/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754).\n",
    "\n",
    "\n",
    "## Relationship Extraction\n",
    "After entity linking to get standardized triples (object, relation, subject) for a knowledge graph, we extract the relationships between the identified entities. \n",
    "The [Rebel project](https://github.com/Babelscape/rebel), which is also available as a spaCy component, allows us to extract both entities and relations in one step, which we can use in our pipeline. \n",
    "\n",
    "To implement our approach of linking the entities to NCIT, we can rewrite the `set_annotations` function from Rebel as specified [here](https://towardsdatascience.com/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754) and turn `call_wiki_api` function into `call_ncit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_W5QS1hGZcw"
   },
   "outputs": [],
   "source": [
    "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "from spacy import Language\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def call_ncit_api(item):\n",
    "  try:\n",
    "    url = f\"https://www.ebi.ac.uk/ols/api/search?q={item}&ontology=ncit\"\n",
    "    data = pd.DataFrame(requests.get(url).json().get('response').get('docs'))\n",
    "    # Return the first id (A simplistic non-perfect way for mapping)\n",
    "    return data[\"label\"][0]\n",
    "  except:\n",
    "    return 'id-less'\n",
    "\n",
    "def extract_triplets(text):\n",
    "    \"\"\"\n",
    "    Function to parse the generated text and extract the triplets\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": 0\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "          Doc.set_extension(\"rel\", default={})\n",
    "\n",
    "    def get_wiki_id(self, item: str):\n",
    "        #mapping = self.entity_mapping.get(item)\n",
    "        #if mapping:\n",
    "        #  return mapping\n",
    "        #else:\n",
    "        res = call_ncit_api(item)\n",
    "        self.entity_mapping[item] = res\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "          output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "          extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "          extracted_triplets = extract_triplets(extracted_text[0])\n",
    "          return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "              continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83TBHk5t0iFW"
   },
   "source": [
    "After redefining the Rebel spaCy component, we include it into our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5wngUkAG8EZ",
    "outputId": "e13a9028-8514-477f-a3a7-1a60558a9c8b"
   },
   "outputs": [],
   "source": [
    "DEVICE = -1 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})\n",
    "\n",
    "# Define rel extraction model\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", config={\n",
    "    'device':DEVICE, # Number of the GPU, -1 if want to use CPU\n",
    "    'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRJY4GV90or6"
   },
   "source": [
    "Now we can text the pipeline on two simple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDEa7ZcpHLeL",
    "outputId": "0a097f91-dbba-4496-957d-92c1904cb17a"
   },
   "outputs": [],
   "source": [
    "input_text = \"High fever is very dangerous. It can be treated with paracetamol.\"\n",
    "\n",
    "coref_text = coref(input_text)._.resolved_text\n",
    "print(coref_text)\n",
    "\n",
    "doc = rel_ext(coref_text)\n",
    "\n",
    "for value, rel_dict in doc._.rel.items():\n",
    "    print(f\"{value}: {rel_dict}\")\n",
    "\n",
    "#output:\n",
    "#0440ea848947d2677bc11443f99f20f67ce0a1bc: {'relation': 'subclass of', 'head_span': {'text': 'High fever', 'id': 'High Grade Fever'}, 'tail_span': {'text': 'dangerous', 'id': 'DRRI-2 - A: Dangerous Military Duties'}}\n",
    "#8aa25d264897bd007d389890b2239c2b9c07fa0b: {'relation': 'drug used for treatment', 'head_span': {'text': 'High fever', 'id': 'High Grade Fever'}, 'tail_span': {'text': 'paracetamol', 'id': 'Acetaminophen Measurement'}}\n",
    "#d91bef9bfc94439523675b5d6a62e1f4635c0cdd: {'relation': 'medical condition treated', 'head_span': {'text': 'paracetamol', 'id': 'Acetaminophen Measurement'}, 'tail_span': {'text': 'High fever', 'id': 'High Grade Fever'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Mm7dzg3-VyfL",
    "outputId": "dfe18ea5-46ca-49b1-9d79-ab5b0e78292c"
   },
   "outputs": [],
   "source": [
    "coref_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZTncR9701CT"
   },
   "source": [
    "You can see, that on the coreference step the \"it\" pronoun in the second sentance was replaced by the unambiguous \"High fever\" entity. \n",
    "After that the rebel model has extracted the trios of subject, relation and object and mapped them to the NCIT model. Note, that the mapping here is far from perfect. For example, the entity 'dangerous' was mapped to the 'DRRI-2 - A: Dangerous Military Duties' in NCIT. this is because in our mapping procedure for simplisity we have chosen the first result for the term in the NCIT database. To improve this, one would need to develop a more complex mapping algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlCdC-S004gr"
   },
   "source": [
    "## Storing the results\n",
    "\n",
    "The final subject, relation, and object trios can be stored as either a labeled property graph or as an RDF graph. The guidelines to store the results as a neo4j labeled property graph are given [here](https://towardsdatascience.com/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754). Here we will give an approach to store the results as an RDF graph by using rdflib library in python.\n",
    "\n",
    "Rdflib allows the creation of entities with known URIs with the URIRef command. Also, one can create a custom namespace with new entities and relations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zsqo11NbaWGO",
    "outputId": "a5acc5be-b931-4639-c329-31f86749e0aa"
   },
   "outputs": [],
   "source": [
    "!pip install rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icbWLysmJcAd",
    "outputId": "e396bd0d-5d33-4416-c7a9-833920a8207d"
   },
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from rdflib import URIRef, BNode, Literal, Namespace\n",
    "import json\n",
    "\n",
    "def Capitalise_underscore(relation):\n",
    "  return relation.capitalize().replace(' ','_')\n",
    "\n",
    "def ncit_iri(item):\n",
    "  try:\n",
    "    url = f\"https://www.ebi.ac.uk/ols/api/search?q={item}&ontology=ncit\"\n",
    "    data = pd.DataFrame(requests.get(url).json().get('response').get('docs'))\n",
    "    # Return the first id\n",
    "    return data[\"iri\"][0]\n",
    "  except:\n",
    "    return 'id-less'\n",
    "\n",
    "g=Graph()\n",
    "EX = Namespace('http://example.org./')\n",
    "\n",
    "relations = pd.DataFrame()\n",
    "\n",
    "\n",
    "results = search('cardiac amyloidosis')\n",
    "id_list = results['IdList']\n",
    "papers = fetch_details(id_list)\n",
    "for i, paper in enumerate(papers['PubmedArticle']):\n",
    "    print(\"{}) {}\".format(i+1, paper['MedlineCitation']['Article']['ArticleTitle']))\n",
    "    \n",
    "    abstract_text_json = json.dumps(papers['PubmedArticle'][i]['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "    abstract_text = ' '.join(json.loads(abstract_text_json))\n",
    "    print(abstract_text)\n",
    "    coref_text = coref(abstract_text)._.resolved_text\n",
    "    doc = rel_ext(coref_text)\n",
    "    for value, rel_dict in doc._.rel.items():\n",
    "      subject_iri = ncit_iri(rel_dict['head_span']['text'])\n",
    "      object_iri = ncit_iri(rel_dict['head_span']['text'])\n",
    "      if subject_iri != 'id=less':\n",
    "        subj = URIRef(subject_iri)\n",
    "      else:\n",
    "        subj = EX[rel_dict['head_span']['text']]\n",
    "      if object_iri != 'id=less':\n",
    "        obj = URIRef(object_iri)\n",
    "      else:\n",
    "        subj = EX[rel_dict['head_span']['text']]  \n",
    "      pred = EX[Capitalise_underscore(rel_dict['relation'])]\n",
    "      g.add((subj, pred, obj))        \n",
    "    df = pd.DataFrame.from_dict(doc._.rel).transpose()\n",
    "    df['subject_text'] = df.head_span.apply(lambda x: x['text'])\n",
    "    df['subject_id'] = df.head_span.apply(lambda x: x['id'])\n",
    "    df['object_text'] = df.tail_span.apply(lambda x: x['text'])\n",
    "    df['object_id'] = df.tail_span.apply(lambda x: x['id'])\n",
    "\n",
    "    df = df.drop([\"head_span\", \"tail_span\"], axis = 1)\n",
    "\n",
    "    relations = pd.concat([relations, df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2MSaNLD1UkV"
   },
   "source": [
    "Finally, we can visualize the resulting graph and export it in .ttl format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vCCg0I61TzG",
    "outputId": "2e5cfb47-4b75-483e-9cb0-fca0794d0feb"
   },
   "outputs": [],
   "source": [
    "print(g.serialize(format = 'ttl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqtraY1i16yX"
   },
   "source": [
    "Visualization of the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "id": "uPLNnapsaT8R",
    "outputId": "e21fcc54-b087-47fe-f6d8-2097ae588e2e"
   },
   "outputs": [],
   "source": [
    "import networkx as ntx\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "graph = ntx.from_pandas_edgelist(relations, \"subject_text\", \"object_text\", edge_attr=True, create_using=ntx.MultiDiGraph())\n",
    "\n",
    "plot.figure(figsize=(10, 10))\n",
    "posn = ntx.spring_layout(graph)\n",
    "ntx.draw(graph, with_labels=True, node_color='green', edge_cmap=plot.cm.Blues, pos = posn)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MgjY1-oijip"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}